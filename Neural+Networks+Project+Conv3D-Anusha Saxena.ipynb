{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWswvFz2B4b3"
   },
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2dFdbLpPB4b9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio.v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio.v2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import imageio.v2 as imageio\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3xk5zTXB4cS"
   },
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W0ivMmOpB4cU",
    "outputId": "de5c2eb9-9dd4-468c-e2ff-680eb77022f2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m rn\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4RWHE-fB4cW"
   },
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WN8glHaLB4cY"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Project_data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_doc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProject_data/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadlines())\n\u001b[1;32m      2\u001b[0m val_doc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject_data/val.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreadlines())\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Project_data/train.csv'"
     ]
    }
   ],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running on local CPU.. hence batch size fixed at 32.\n",
    "    - Sequence (total videos 663) and # of full Batch (663/batch size 32 = 20) \n",
    "    - # of Videos in final batch (left over videos after full batchs : 663 minus 640 = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Shuffle the order of Videos (not frames)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[43mtrain_doc\u001b[49m)\n\u001b[1;32m      4\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      5\u001b[0m final_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(t)\u001b[38;5;241m%\u001b[39mbatch_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_doc' is not defined"
     ]
    }
   ],
   "source": [
    "# Shuffle the order of Videos (not frames)\n",
    "\n",
    "t = np.random.permutation(train_doc)\n",
    "num_batches = int(len(t)/32)\n",
    "final_batch = len(t)%batch_size\n",
    "print(len(t))   #  Sequence - # of videos\n",
    "print(num_batches) \n",
    "print(final_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images (video resolution) of different size (360x360, 120x160), Resized to smaller resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6bLU5_9B4cZ"
   },
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with some of the parts of the generator function such that you get high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uC_ksZ3XB4ca"
   },
   "outputs": [],
   "source": [
    "def generator_8(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "# choose the required number of images from each video - more the images considered longer the training period\n",
    "    img_idx = [1, 5, 10, 15, 20, 22, 25, 29 ]  \n",
    "\n",
    "# generate data till the last epoch\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "\n",
    "# batch_data holds the number of images mentioned in batch size\n",
    "            batch_data = np.zeros((batch_size,8,84,84,3)) # batch_size, 3D resolution, channel \n",
    "\n",
    "# batch label holds the class of the image corresponding to the image in batch_data\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                \n",
    "# List all Image file names in the current folder \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                #print(imgs)\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "# crop : only the 120x160 images will be procesesed by the below crop operation\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "\n",
    "# resize : crop operation performed above will convert 120x160 to 120x140.\n",
    "# With the images that are 120x140 fetch 120x120 pixels and resize them to 84x84\n",
    "\n",
    "                    if image.shape[1] == 140:  \n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "# Normalize RGB channel data\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "# apply class label as binary OHT [0. 1. 0. 0. 0.]                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "# Final batch with left over videos after processing all the FULL batches\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,8,84,84,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "                        \n",
    "                        \n",
    "                    if image.shape[1] == 140:\n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OKusyUJcB4cd",
    "outputId": "42d7c299-51a1-49ad-8eb5-cf80f6b949d0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch date/time to create a folder to save h5 files which can later be loaded to test the model performance\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m curr_dt_time \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      5\u001b[0m train_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject_data/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m val_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProject_data/val\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# Fetch date/time to create a folder to save h5 files which can later be loaded to test the model performance\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "source_path=train_path\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OELVUTqBB4ce"
   },
   "source": [
    "## Conv3D Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D`. Also remember that the last layer is the softmax. Remember that the network is designed in such a way that the model is able to fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebqwqV_yB4cf"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden Layer 1\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(8,84,84,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 3\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 4\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense to the 5 gesture classes\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4yeetfCB4ct"
   },
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI0za7kgB4c2",
    "outputId": "09469b94-2990-437b-c2de-6156c8547991"
   },
   "outputs": [],
   "source": [
    "# Setting faster learning rate\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "\n",
    "#Compile Model\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nU3mGWOB4c6"
   },
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Yp3XgkbB4c7"
   },
   "outputs": [],
   "source": [
    "train_generator = generator_8(train_path, train_doc, batch_size)\n",
    "val_generator = generator_8(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdAfbtLWB4c7",
    "outputId": "1715607d-6400-4231-db75-eeee50068dce"
   },
   "outputs": [],
   "source": [
    "# Create folder to save H5 files generated for each epoch (save_best_only=False)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "# Save H5 file with loss, acc, val loss, val acc metrics\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "# check point : save H5 file after each epoch\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Reduce Learning Rate (overfitting) when model Platueaus \n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.001, cooldown=0, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SxnCT9IB4c8"
   },
   "outputs": [],
   "source": [
    "# Training Epoch - # of steps. Increment the step value by one when there is residual videos after FULL batch\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "# Validation Epoch - # of steps\n",
    "    \n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZUamCNJB4dA"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fCt_eSFB4dM",
    "outputId": "dec7c59f-5346-43db-f9d5-a8f45703da14",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more images to the training ( 8 to 18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC_ksZ3XB4ca"
   },
   "outputs": [],
   "source": [
    "def generator_18(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "\n",
    "# choose the required number of images from each video - more the images considered longer the training period\n",
    "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29] # [1, 5, 10, 15, 20, 22, 25, 29 ]  \n",
    "\n",
    "# generate data till the last epoch\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "\n",
    "# batch_data holds the number of images mentioned in batch size\n",
    "            batch_data = np.zeros((batch_size,18,84,84,3)) # batch_size, 3D resolution, channel \n",
    "\n",
    "# batch label holds the class of the image corresponding to the image in batch_data\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                \n",
    "# List all Image file names in the current folder \n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                #print(imgs)\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "# crop : only the 120x160 images will be procesesed by the below crop operation\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "\n",
    "# resize : crop operation performed above will convert 120x160 to 120x140.\n",
    "# With the images that are 120x140 fetch 120x120 pixels and resize them to 84x84\n",
    "\n",
    "                    if image.shape[1] == 140:  \n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "# Normalize RGB channel data\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "# apply class label as binary OHT [0. 1. 0. 0. 0.]                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "# Final batch with left over videos after processing all the FULL batches\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,18,84,84,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = imageio.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    if image.shape[0] != image.shape[1]:\n",
    "                        image = image[0:120, 10:150]\n",
    "                    else:\n",
    "                        image = image\n",
    "                        \n",
    "                        \n",
    "                    if image.shape[1] == 140:\n",
    "                        image = resize(image[:,10:130,:],(84,84)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(84,84)).astype(np.float32)\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKusyUJcB4cd",
    "outputId": "42d7c299-51a1-49ad-8eb5-cf80f6b949d0"
   },
   "outputs": [],
   "source": [
    "# Fetch date/time to create a folder to save h5 files which can later be loaded to test the model performance\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "source_path=train_path\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator_18(train_path, train_doc, batch_size)\n",
    "val_generator = generator_18(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save H5 files generated for each epoch (save_best_only=False)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "# Save H5 file with loss, acc, val loss, val acc metrics\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "# check point : save H5 file after each epoch\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Reduce Learning Rate (overfitting) when model Platueaus \n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.001, cooldown=0, min_lr=0.0001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Hidden Layer 1\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,84,84,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "# Hidden Layer 2\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 3\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden Layer 4\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense to the 5 gesture classes\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting faster learning rate\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "\n",
    "#Compile Model\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit_generator(train_generator, steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      2\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39mcallbacks_list, \n\u001b[1;32m      3\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39mval_generator, \n\u001b[1;32m      4\u001b[0m                     validation_steps\u001b[38;5;241m=\u001b[39mvalidation_steps, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, initial_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m                    )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural+Networks+Project+Conv3D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
